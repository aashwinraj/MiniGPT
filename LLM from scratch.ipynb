{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cc1f8fd-a6fb-4e2c-9b5a-b27ed1a996a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Access is denied.\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1ed55d7-23b8-49a5-89de-226331b3c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "data=load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06b474ab-6d1c-4c97-aabd-4466ea169389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20085543-4ecc-4b94-97c3-be68b7db586d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.',\n",
       " 'Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong.\\n\\nOne day, Beep was driving in the park when he saw a big tree. The tree had many leaves that were falling. Beep liked how the leaves fall and wanted to play with them. Beep drove under the tree and watched the leaves fall on him. He laughed and beeped his horn.\\n\\nBeep played with the falling leaves all day. When it was time to go home, Beep knew he needed more fuel. He went to the fuel place and got more healthy fuel. Now, Beep was ready to go fast and play again the next day. And Beep lived happily ever after.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts=[data[\"train\"][i]['text']for i in range(2)]\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8677bf-f15a-45e6-96d2-f3c6484ba9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60a23eac-7ff8-4a5e-af49-8f065c53cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "BLOCK_SIZE=256\n",
    "VOCAB_SIZE=len(tokenizer)\n",
    "BLOCK_SIZE=256\n",
    "D_MODEL=512\n",
    "N_HEADS=16\n",
    "N_LAYERS=16\n",
    "DFF=4*512\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"],truncation=True,max_length=BLOCK_SIZE,padding=\"max_length\")\n",
    "tokenized_dataset=data.map(tokenize,batched=True)                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f314db44-6e61-4022-b18f-79a67d8a9577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([8, 255])\n",
      "Target: torch.Size([8, 255])\n",
      "Example IDs: tensor([14967,   290,  3409,   389,  2460,    13,  1119,   588,   284,   711])\n",
      "Decoded: Tim and Sam are friends. They like to play with their kayaks in the lake. They have a race to see who is faster.\n",
      "\n",
      "\"Ready, set, go!\" Tim says. He paddles hard with his stick. Sam paddles hard too. They splash water and laugh.\n",
      "\n",
      "Tim is in front. He sees a big rock. He thinks he can go around it. He is smart. He does not want to hit the rock.\n",
      "\n",
      "But Sam does not see the rock. He paddles straight to it. He hits the rock with his kayak. His kayak stops. He is sad.\n",
      "\n",
      "Tim looks back. He sees Sam and the rock. He stops too. He is kind. He does not want to win alone.\n",
      "\n",
      "\"Are you okay, Sam?\" Tim asks. He paddles back to Sam.\n",
      "\n",
      "\"I'm okay, Tim. But my kayak is stuck. I can't move it.\" Sam says.\n",
      "\n",
      "Tim helps Sam. He pushes the rock with his stick. The rock moves a little. Sam pulls his kayak. The kayak comes out. Sam is happy.\n",
      "\n",
      "\"Thank you, Tim. You are a good friend. You are smart\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self,data):\n",
    "        self.data=data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,idx):\n",
    "        ids=torch.tensor(self.data[idx]['input_ids'],dtype=torch.long)\n",
    "        x=ids[:-1]\n",
    "        y=ids[1:]\n",
    "        return x,y\n",
    "\n",
    "train_data=GPTDataset(tokenized_dataset['train'])\n",
    "train_data_loader=DataLoader(train_data,batch_size=8,shuffle=True)\n",
    "for x, y in train_data_loader:\n",
    "    print(\"Input:\", x.shape)\n",
    "    print(\"Target:\", y.shape)\n",
    "    print(\"Example IDs:\", x[0][:10])\n",
    "    print(\"Decoded:\", tokenizer.decode(x[0]))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77441e6b-7f47-4328-933e-bacc9eaab2f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3138f560-0044-4e9b-9077-c6261f8a77ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalLM(nn.Module):\n",
    "    def __init__(self,n_head,d_model,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.n_head=n_head\n",
    "        self.dk=d_model//n_head\n",
    "        self.qkv=nn.Linear(d_model,3*d_model,bias=False)\n",
    "        self.out_proj=nn.Linear(d_model,d_model,bias=False)\n",
    "        self.proj_dropout=nn.Dropout(dropout)\n",
    "        self.attn_dropout=nn.Dropout(dropout)\n",
    "    def build_causal_mask(self,T,device):\n",
    "        mask = torch.tril(torch.ones((T, T), dtype=torch.bool, device=device))\n",
    "        # we'll use it to set -inf on disallowed positions\n",
    "        return mask.unsqueeze(0).unsqueeze(0)\n",
    "    def forward(self,x):\n",
    "        B,T,D=x.shape\n",
    "        qkv=self.qkv(x)\n",
    "        q,k,v=qkv.chunk(3,-1)\n",
    "        q=q.view(B,T,self.n_head,self.dk).transpose(1,2)\n",
    "        k=k.view(B,T,self.n_head,self.dk).transpose(1,2)\n",
    "        v=v.view(B,T,self.n_head,self.dk).transpose(1,2)\n",
    "        scores=torch.matmul(q,k.transpose(-2,-1))\n",
    "        scores=scores/(self.dk**0.5)\n",
    "        self.causal_mask=self.build_causal_mask(T,x.device)\n",
    "        causal=self.causal_mask[:,:,:T,:T]\n",
    "        scores = scores.masked_fill(~causal, float(\"-inf\"))\n",
    "        attn_weights=f.softmax(scores,dim=-1)\n",
    "        attn_weights=self.attn_dropout(attn_weights)\n",
    "        context=torch.matmul(attn_weights,v)\n",
    "        context=context.transpose(1,2).contiguous().view(B,T,D)\n",
    "        out=self.out_proj(context)\n",
    "        out=self.proj_dropout(out)\n",
    "        return out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55832cc8-40bc-44f6-a4ec-fb01c32fa26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,ff_dim,d_model,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(nn.Linear(d_model,ff_dim),\n",
    "                               nn.GELU(),\n",
    "                               nn.Linear(ff_dim,d_model),\n",
    "                               nn.Dropout(dropout)\n",
    "                              )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b01190f5-6f7b-4b62-8db9-694a2aa48702",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,d_model,n_heads,dff,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1=nn.LayerNorm(d_model)\n",
    "        self.attn=CausalLM(n_heads,d_model)\n",
    "        self.ln2=nn.LayerNorm(d_model)\n",
    "        self.ff=FeedForward(dff,d_model,dropout)\n",
    "    def forward(self,x):\n",
    "        x=x+self.attn(self.ln1(x))\n",
    "        x=x+self.ff(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c19ad123-b201-4592-ab4c-be7c993f2c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self,vocab_size,block_size,d_model,n_head,n_layers):\n",
    "        super().__init__()\n",
    "        self.token_emb=nn.Embedding(vocab_size,d_model)\n",
    "        self.pos_emb=nn.Embedding(block_size,d_model)\n",
    "        self.blocks=nn.ModuleList([\n",
    "            TransformerBlock(d_model,n_head,dff=4*d_model)\n",
    "        ])\n",
    "        self.lnf=nn.LayerNorm(d_model)\n",
    "        self.head=nn.Linear(d_model,vocab_size,bias=False)\n",
    "        self.block_size=block_size\n",
    "        self.vocab_size=vocab_size\n",
    "    def forward(self,idx,targets=None):\n",
    "        B,T=idx.shape\n",
    "        token_emb=self.token_emb(idx)\n",
    "        pos=torch.arange(T,device=idx.device)\n",
    "        pos_emb=self.pos_emb(pos)\n",
    "        x=token_emb+pos_emb\n",
    "        for block in self.blocks:\n",
    "            x = block(x)                                           # apply transformer block\n",
    "        x = self.lnf(x)                                           # final norm\n",
    "        logits = self.head(x) \n",
    "        if targets!=None:\n",
    "            loss = f.cross_entropy(logits.view(-1, self.vocab_size), targets.view(-1))\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits,0\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a627161-5815-4d86-b965-3b549a84cdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (token_emb): Embedding(50257, 512)\n",
       "  (pos_emb): Embedding(256, 512)\n",
       "  (blocks): ModuleList(\n",
       "    (0): TransformerBlock(\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalLM(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (proj_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lnf): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=512, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(VOCAB_SIZE)\n",
    "model=GPT(VOCAB_SIZE,BLOCK_SIZE,D_MODEL,N_HEADS,N_LAYERS)\n",
    "optimizer=torch.optim.AdamW(model.parameters(),lr=3e-4)\n",
    "epochs=3\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51447237-c6e4-42dc-97e0-dd0257a68b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Step 0 | Loss: 10.9174\n",
      "Epoch 0 | Step 100 | Loss: 4.2639\n",
      "Epoch 0 | Step 200 | Loss: 3.6466\n",
      "Epoch 0 | Step 300 | Loss: 3.3378\n",
      "Epoch 0 | Step 400 | Loss: 3.4865\n",
      "Epoch 0 | Step 500 | Loss: 2.7952\n",
      "Epoch 0 | Step 600 | Loss: 2.6103\n",
      "Epoch 0 | Step 700 | Loss: 3.1493\n",
      "Epoch 0 | Step 800 | Loss: 3.0161\n",
      "Epoch 0 | Step 900 | Loss: 2.7836\n",
      "Epoch 1 | Step 0 | Loss: 2.9910\n",
      "Epoch 1 | Step 100 | Loss: 2.7745\n",
      "Epoch 1 | Step 200 | Loss: 2.8900\n",
      "Epoch 1 | Step 300 | Loss: 2.8342\n",
      "Epoch 1 | Step 400 | Loss: 2.5117\n",
      "Epoch 1 | Step 500 | Loss: 2.7868\n",
      "Epoch 1 | Step 600 | Loss: 2.7755\n",
      "Epoch 1 | Step 700 | Loss: 2.3086\n",
      "Epoch 1 | Step 800 | Loss: 2.3186\n",
      "Epoch 1 | Step 900 | Loss: 2.6154\n",
      "Epoch 2 | Step 0 | Loss: 2.4877\n",
      "Epoch 2 | Step 100 | Loss: 2.7804\n",
      "Epoch 2 | Step 200 | Loss: 2.6416\n",
      "Epoch 2 | Step 300 | Loss: 2.8494\n",
      "Epoch 2 | Step 400 | Loss: 2.4498\n",
      "Epoch 2 | Step 500 | Loss: 2.3245\n",
      "Epoch 2 | Step 600 | Loss: 2.7600\n",
      "Epoch 2 | Step 700 | Loss: 2.8159\n",
      "Epoch 2 | Step 800 | Loss: 2.2986\n",
      "Epoch 2 | Step 900 | Loss: 2.6253\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'avg_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 20\u001b[0m\n\u001b[0;32m     13\u001b[0m save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint_epoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(save_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     16\u001b[0m torch\u001b[38;5;241m.\u001b[39msave({\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: avg_loss\n\u001b[0;32m     21\u001b[0m }, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     23\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(save_dir)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Model and tokenizer saved in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'avg_loss' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for epoch in range(epochs):\n",
    "    for step, (x,y) in enumerate(train_data_loader):\n",
    "        if step==1000:\n",
    "            break\n",
    "        x,y=x.to(device),y.to(device)\n",
    "        logits,loss=model(x,y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch {epoch} | Step {step} | Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daa1cae7-b19c-491c-8285-bc7ef63b3ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model and tokenizer saved in checkpoint_epoch_3\n"
     ]
    }
   ],
   "source": [
    "save_dir = f\"checkpoint_epoch_{epoch+1}\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "torch.save({\n",
    "    'epoch': epoch + 1,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, os.path.join(save_dir, \"model.pt\"))\n",
    "\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "print(f\"✅ Model and tokenizer saved in {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0ca37c2-b38d-4195-bb54-c3cd2d63c3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def generate(mode,tokenizer,prompt,max_new_tokens):\n",
    "\n",
    "    idx_inf = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Forward pass (causal)\n",
    "        logits, _ = model(idx_inf)\n",
    "        logits = logits[:, -1, :]  # last token only\n",
    "        probs = f.softmax(logits, dim=-1)\n",
    "\n",
    "        # Sample next token\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        idx_inf = torch.cat([idx_inf, next_token], dim=1)\n",
    "\n",
    "    return tokenizer.decode(idx_inf[0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44d4cfe8-a143-4672-b620-384975242e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a boy not very cute boy. He loved to star, more fruits for pictures. Every day he would do his quest. One day, Bob went to a football. He saw thunder and pretended to move to the door. \n",
      "\n",
      "He had fun and\n"
     ]
    }
   ],
   "source": [
    "text = generate(model, tokenizer, \"There was a boy\", max_new_tokens=50)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7d87f9f-719d-4108-a45b-2fab2b1716d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory cleared successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "\n",
    "# Empty PyTorch cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Reset memory stats (optional)\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# Synchronize to make sure all CUDA ops are done\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Clear any dangling CUDA graphs or memory pools (PyTorch 2.0+)\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "print(\"GPU memory cleared successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91deb487-14e8-4fc3-bbd1-73cc951c0874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d7c7bc-2612-49ca-abf1-19e9cfc9e78c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674037d2-2736-4d0d-8e14-4d9cfad0eecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0f2b21-1d21-48cc-9b5f-7c065a2f20c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e71402f-98ae-4903-8197-606123600a49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
